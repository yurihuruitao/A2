{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f53788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hanlp\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# 初始化HanLP分词器\n",
    "tokenizer = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ELECTRA_SMALL_ZH)\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs('30-50_split', exist_ok=True)\n",
    "\n",
    "# 存储每章节的人物\n",
    "chapter_persons = []\n",
    "\n",
    "# 遍历30-50文件夹中的txt文件\n",
    "for filename in sorted(os.listdir('30-50')):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join('30-50', filename)\n",
    "        \n",
    "        # 读取文件\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # 分词\n",
    "        tokens = tokenizer(text)\n",
    "        \n",
    "        # 保存分词结果\n",
    "        output_path = os.path.join('30-50_split', filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(' '.join(tokens))\n",
    "        \n",
    "        # 命名实体识别，找出人物\n",
    "        entities = recognizer(tokens)\n",
    "        persons = []\n",
    "        for entity_tuple in entities:\n",
    "            entity_name = entity_tuple[0]\n",
    "            entity_label = entity_tuple[1]\n",
    "            if entity_label == 'PERSON':\n",
    "                persons.append(entity_name)\n",
    "        \n",
    "        # 统计人物出现频率\n",
    "        person_count = Counter(persons)\n",
    "        \n",
    "        # 获取出现最多的前5个人物\n",
    "        top_persons = [person for person, count in person_count.most_common(5)]\n",
    "        \n",
    "        # 保存章节和人物\n",
    "        chapter_name = filename.replace('.txt', '')\n",
    "        for person in top_persons:\n",
    "            chapter_persons.append([chapter_name, person])\n",
    "\n",
    "# 导出到CSV\n",
    "with open('人物统计.csv', 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['章节', '人物'])\n",
    "    writer.writerows(chapter_persons)\n",
    "\n",
    "print('完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ab9734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取CSV文件\n",
    "chapter_persons = defaultdict(list)\n",
    "\n",
    "with open('人物统计.csv', 'r', encoding='utf-8-sig') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for row in reader:\n",
    "        chapter = row[0]\n",
    "        person = row[1]\n",
    "        # 过滤掉只有一个字的名字\n",
    "        if len(person) > 1:\n",
    "            chapter_persons[chapter].append(person)\n",
    "\n",
    "# 写入新的CSV文件\n",
    "with open('人物统计_合并.csv', 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['章节', '人物'])\n",
    "    for chapter in sorted(chapter_persons.keys()):\n",
    "        persons = ', '.join(chapter_persons[chapter])\n",
    "        writer.writerow([chapter, persons])\n",
    "\n",
    "print('完成！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
